{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117682,"databundleVersionId":14443416,"sourceType":"competition"},{"sourceId":13802080,"sourceType":"datasetVersion","datasetId":8787846},{"sourceId":13818749,"sourceType":"datasetVersion","datasetId":8800020},{"sourceId":14063070,"sourceType":"datasetVersion","datasetId":8945494},{"sourceId":14066894,"sourceType":"datasetVersion","datasetId":8953734},{"sourceId":284794863,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index --find-links=\"/kaggle/input/surface-package-scraper\" -q pytorch_lightning monai albumentations imagecodecs --no-deps # \"numpy==1.26.4\" \"scipy==1.15.3\"\n!pip uninstall -q -y tensorflow  # preventing AttributeError","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:16.145032Z","iopub.execute_input":"2025-12-10T02:58:16.145409Z","iopub.status.idle":"2025-12-10T02:58:26.422198Z","shell.execute_reply.started":"2025-12-10T02:58:16.145381Z","shell.execute_reply":"2025-12-10T02:58:26.420936Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 1\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:26.424386Z","iopub.execute_input":"2025-12-10T02:58:26.424677Z","iopub.status.idle":"2025-12-10T02:58:26.430182Z","shell.execute_reply.started":"2025-12-10T02:58:26.424647Z","shell.execute_reply":"2025-12-10T02:58:26.429009Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"1 - 384\n\n\n40 - 256\n\n750 - 320","metadata":{"execution":{"iopub.status.busy":"2025-12-04T06:40:26.996809Z","iopub.execute_input":"2025-12-04T06:40:26.997143Z","iopub.status.idle":"2025-12-04T06:40:27.005267Z","shell.execute_reply.started":"2025-12-04T06:40:26.997122Z","shell.execute_reply":"2025-12-04T06:40:27.004282Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport gc\nimport warnings\nfrom typing import Tuple, Optional, Dict, List, Callable\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport os\nwarnings.filterwarnings(\"ignore\")\n\nimages_path = \"/kaggle/input/vesuvius-surface-npz/train_images\"\nmask_path = \"/kaggle/input/vesuvius-surface-npz/train_labels\"\nroot_dir = \"/kaggle/input/vesuvius-surface-npz\"\ntest_images_path = \"/kaggle/input/vesuvius-challenge-surface-detection/test_images\"\nOUTPUT_DIR = \"/kaggle/working/checkpoints\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:26.431204Z","iopub.execute_input":"2025-12-10T02:58:26.431573Z","iopub.status.idle":"2025-12-10T02:58:31.892059Z","shell.execute_reply.started":"2025-12-10T02:58:26.431543Z","shell.execute_reply":"2025-12-10T02:58:31.889459Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dill/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# the package is installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__info__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__author__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__license__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_cache_bytecode\u001b[0;34m(self, source_path, bytecode_path, data)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, path, data, _mode)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_write_atomic\u001b[0;34m(path, data, mode)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2792264883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from torch.nn import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m from .activation import (\n\u001b[1;32m      4\u001b[0m     \u001b[0mCELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceLikeType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackwardHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.utils import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbackcompat\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbackcompat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcollect_env\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcollect_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torch.utils.data.dataloader import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdefault_collate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdefault_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/graph_settings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.utils.data.datapipes.iter.sharding import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0m_ShardingIterDataPipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mSHARDING_PRIORITIES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/dataframe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torch.utils.data.datapipes.dataframe.dataframes import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCaptureDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mDFIterDataPipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFramesAsTuplesPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/dataframe/dataframes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataChunkDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFIterDataPipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/_decorator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_DataPipeMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapDataPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/datapipe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mHAS_DILL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_import_utils.py\u001b[0m in \u001b[0;36mimport_dill\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# XXX: By default, dill writes the Pickler dispatch table to inject its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dill/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# get distribution meta info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     from version import (__version__, __author__,\n\u001b[0m\u001b[1;32m     19\u001b[0m                          get_license_text, get_readme_as_rst)\n\u001b[1;32m     20\u001b[0m     \u001b[0m__license__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_license_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LICENSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'version'"],"ename":"ModuleNotFoundError","evalue":"No module named 'version'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.lr = 1e-4\n        self.num_workers = 2\n        self.batches = 1\n        self.val_split = 0.2\n        self.target_shape = (128,128,128)\n        self.weight_decay = 2e-4\n        self.max_epochs = 50\n        \n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.892822Z","iopub.status.idle":"2025-12-10T02:58:31.893115Z","shell.execute_reply.started":"2025-12-10T02:58:31.892979Z","shell.execute_reply":"2025-12-10T02:58:31.892991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom pathlib import Path\nimport tifffile\nimport pytorch_lightning as pl\nimport random\nfrom monai import transforms as MT\nclass VesuviusDataset(Dataset):\n    def __init__(self, \n                 images_id: list, \n                 images_dir: str = images_path, \n                 test_dir: str = test_images_path,   \n                 masks_dir: str = mask_path, \n                 mode: str = \"train\"):\n        \n        self.images_id = images_id\n        self.mode = mode\n        self.images_dir = Path(images_dir)\n        self.masks_dir = Path(masks_dir) if masks_dir else None\n        self.test_dir = Path(test_dir) if test_dir else None\n\n    def __len__(self):\n        return len(self.images_id)\n\n    def __getitem__(self, idx):\n        file_name = self.images_id[idx]\n        \n        # 1. Determine Source Directory\n        if self.mode == \"test\":\n            if self.test_dir is None:\n                raise ValueError(\"Mode is 'test' but no 'test_dir' was provided!\")\n            source_dir = self.test_dir\n        else:\n            # Both 'train' and 'val' come from the training images directory\n            source_dir = self.images_dir\n\n        # 2. Load Image\n        img_path = source_dir / file_name\n        img = self.load_volume(img_path)\n\n        # 3. Load Mask (For both Train AND Val)\n        if self.mode in [\"train\", \"val\"]:\n            if self.masks_dir is None:\n                raise ValueError(f\"Mode is '{self.mode}' but no 'masks_dir' was provided!\")\n            \n            mask_path = self.masks_dir / file_name\n            mask = self.load_volume(mask_path, is_mask=True)\n            return img, mask, file_name\n            \n        # 4. Test mode (No mask)\n        else:\n            return img, None, file_name\n\n    def load_volume(self, file_path, is_mask=False):\n        path_obj = Path(file_path)\n        if not path_obj.exists():\n            raise FileNotFoundError(f\"Could not find file: {path_obj}\")\n        if path_obj.suffix == \".npz\":\n            archive = np.load(str(path_obj))\n            data = archive[list(archive.files)[0]]\n        elif path_obj.suffix in [\".tif\", \".tiff\"]:\n            data = tifffile.imread(str(path_obj))\n        else:\n            data = np.load(str(path_obj))\n        tensor = torch.from_numpy(data)\n\n        if not is_mask:\n            tensor = tensor.half().div_(255.0).unsqueeze(0)\n        else:\n            tensor = tensor.long().unsqueeze(0)\n\n        return tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.894121Z","iopub.status.idle":"2025-12-10T02:58:31.894487Z","shell.execute_reply.started":"2025-12-10T02:58:31.894290Z","shell.execute_reply":"2025-12-10T02:58:31.894305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimages_id = None\nfor _,_,c in os.walk(images_path):\n    images_id = c\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.895822Z","iopub.status.idle":"2025-12-10T02:58:31.896096Z","shell.execute_reply.started":"2025-12-10T02:58:31.895958Z","shell.execute_reply":"2025-12-10T02:58:31.895971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate(batch):\n    return batch\n#we want colation on happen on gpu as cpu cant do heavy interpolation\n#skipping the collation in dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.897723Z","iopub.status.idle":"2025-12-10T02:58:31.898071Z","shell.execute_reply.started":"2025-12-10T02:58:31.897889Z","shell.execute_reply":"2025-12-10T02:58:31.897907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning \nclass DesuviusDataModule(pl.LightningDataModule):\n\n    def __init__(self,\n                 train_img_dir = images_path,\n                 test_img_dir = test_images_path,\n                 mask_img_dir = mask_path,\n                 num_workers = config.num_workers,\n                 batches = config.batches,\n                 target_shape = config.target_shape):\n        super().__init__()\n        self.train_img_dir = train_img_dir\n        self.test_img_dir= test_img_dir\n        self.mask_img_dir = mask_img_dir\n        self.num_workers = num_workers\n        self.batches = batches\n        self.target_shape = target_shape\n        self.val_split = config.val_split\n\n        self.train_dataset = None\n        self.test_dataset = None\n        self.mask_dataset = None\n\n        self.gpu_augments = MT.Compose([\n            MT.Resized(keys=[\"image\", \"label\"], spatial_size=self.target_shape, mode=[\"trilinear\", \"nearest\"]),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n            MT.RandRotated(keys=[\"image\", \"label\"], range_x=0.1, range_y=0.1, range_z=0.1, prob=0.3, keep_size=True, mode=[\"bilinear\", \"nearest\"]),\n            MT.RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n            MT.RandGaussianNoised(keys=[\"image\"], prob=0.3, mean=0.0, std=0.01),\n        ])\n        # Validation transforms: Just Resize (for image AND label)\n        self.val_augments = MT.Compose([\n            MT.Resized(keys=[\"image\", \"label\"], spatial_size=self.target_shape, mode=[\"trilinear\", \"nearest\"])\n        ])\n        # Validation transforms for Image ONLY (for test set where labels are None)\n        self.val_image_augments = MT.Compose([\n            MT.Resized(keys=[\"image\"], spatial_size=self.target_shape, mode=[\"trilinear\"])\n        ])\n\n    def setup(self, stage = None):\n        random.seed(32)\n        all_files = images_id # Assumes images_id is defined globally\n        random.shuffle(all_files)\n        # Split into train/val\n        split_idx = int(len(all_files) * (1 - self.val_split))\n        train_files = all_files[:split_idx]\n        val_files = all_files[split_idx:]\n        print(f\"Total files: {len(all_files)}\")\n        print(f\"Train files: {len(train_files)}\")\n        print(f\"Val files: {len(val_files)}\")\n\n        # FIX: Use the split files, not the global list\n        self.train_dataset = VesuviusDataset(images_id = train_files, mode = \"train\")\n        self.val_dataset = VesuviusDataset(images_id = val_files, mode = \"val\")\n\n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.train_dataset,\n            batch_size = self.batches,\n            shuffle = True,\n            num_workers = self.num_workers,\n            pin_memory = True,\n            persistent_workers=bool(self.num_workers > 0),\n            collate_fn = custom_collate\n        )\n    def val_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.val_dataset,\n            batch_size = self.batches,\n            shuffle = False, # FIX: Should be False for validation\n            num_workers = self.num_workers,\n            pin_memory = True,\n            persistent_workers=bool(self.num_workers > 0),\n            collate_fn = custom_collate\n        )\n\n    def on_after_batch_transfer(self, batch, dataloader_idx): \n        if not isinstance(batch, list):\n            return super().on_after_batch_transfer(batch, dataloader_idx)\n\n        x_list, y_list, frag_ids = [], [], [] # FIX: renamed z_list to frag_ids\n        \n        # Determine device\n        device = self.trainer.strategy.root_device if self.trainer else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        for item in batch:\n            x, y, frag_id = item\n            x = x.to(device, non_blocking=True)\n            \n            # Logic: If we are training, use gpu_augments. \n            # If validating and y exists, use val_augments.\n            # If y is None (predict), use val_image_augments.\n            \n            if y is not None:\n                y = y.to(device, non_blocking=True)\n                data = {\"image\": x, \"label\": y}\n                \n                # Pick transform\n                if self.trainer.training:\n                    trans = self.gpu_augments\n                else:\n                    trans = self.val_augments\n                \n                data = trans(data) # FIX: Use selected transform\n                y_list.append(data[\"label\"])\n            else:\n                # Prediction case (No Label)\n                data = {\"image\": x}\n                trans = self.val_image_augments\n                data = trans(data)\n                # Do not append to y_list\n            \n            x_list.append(data[\"image\"])\n            frag_ids.append(frag_id)\n\n        # Handle Y stack (if empty)\n        y_stack = torch.stack(y_list) if len(y_list) > 0 else None\n        \n        return torch.stack(x_list), y_stack, frag_ids # FIX: Return list frag_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.899943Z","iopub.status.idle":"2025-12-10T02:58:31.900617Z","shell.execute_reply.started":"2025-12-10T02:58:31.900420Z","shell.execute_reply":"2025-12-10T02:58:31.900444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataModule = DesuviusDataModule()\ndataModule.setup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.902183Z","iopub.status.idle":"2025-12-10T02:58:31.902569Z","shell.execute_reply.started":"2025-12-10T02:58:31.902407Z","shell.execute_reply":"2025-12-10T02:58:31.902426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataModule.train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.904264Z","iopub.status.idle":"2025-12-10T02:58:31.904631Z","shell.execute_reply.started":"2025-12-10T02:58:31.904463Z","shell.execute_reply":"2025-12-10T02:58:31.904478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get a batch\ntrain_loader = dataModule.train_dataloader()\nbatch = next(iter(train_loader))\n\n# With custom_collate, batch is a list: [(img, mask, id), ...]\n# We extract the first sample\nraw_img, raw_mask, frag_id = batch[0]\n\nprint(f\"Raw ID: {frag_id}\")\nprint(f\"Raw Image Shape: {raw_img.shape}\")\n\n# Manually apply the validation transform (Resize) to visualize the model input\n# We use val_augments which only contains Resized\n# Construct dictionary as expected by MONAI transforms\ndata = {\"image\": raw_img, \"label\": raw_mask}\ndata_resized = dataModule.val_augments(data)\n\nimages = data_resized[\"image\"]\nmasks = data_resized[\"label\"]\n\n# Select first channel\nimg = images[0].numpy()  # (D, H, W)\nmsk = masks[0].numpy()   # (D, H, W)\n\nprint(f\"Resized Image Shape: {img.shape}\")\nprint(f\"Mask Shape: {msk.shape}\")\nprint(f\"Image Range: {img.min()} - {img.max()}\")\n\n# Calculate middle indices\nd_mid = img.shape[0] // 2\nh_mid = img.shape[1] // 2\nw_mid = img.shape[2] // 2\n\n# Setup plot: 3 Rows (Axes), 2 Cols (Image, Mask)\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\n# Row 1: Z-axis (Depth/Axial)\naxes[0, 0].imshow(img[d_mid, :, :], cmap='gray')\naxes[0, 0].set_title(f'Axial (Depth={d_mid}) - Image')\naxes[0, 1].imshow(msk[d_mid, :, :], cmap='gray')\naxes[0, 1].set_title(f'Axial (Depth={d_mid}) - Mask')\n\n# Row 2: Y-axis (Height/Coronal)\naxes[1, 0].imshow(img[:, h_mid, :], cmap='gray')\naxes[1, 0].set_title(f'Coronal (Height={h_mid}) - Image')\naxes[1, 1].imshow(msk[:, h_mid, :], cmap='gray')\naxes[1, 1].set_title(f'Coronal (Height={h_mid}) - Mask')\n\n# Row 3: X-axis (Width/Sagittal)\naxes[2, 0].imshow(img[:, :, w_mid], cmap='gray')\naxes[2, 0].set_title(f'Sagittal (Width={w_mid}) - Image')\naxes[2, 1].imshow(msk[:, :, w_mid], cmap='gray')\naxes[2, 1].set_title(f'Sagittal (Width={w_mid}) - Mask')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.905619Z","iopub.status.idle":"2025-12-10T02:58:31.905882Z","shell.execute_reply.started":"2025-12-10T02:58:31.905757Z","shell.execute_reply":"2025-12-10T02:58:31.905769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pytorch_lightning as pl\nfrom monai.losses import TverskyLoss\n\nclass surfaceSegementation(pl.LightningModule):\n    def __init__(self, net, learning_rate=1e-3, weight_decay=1e-4):\n        super().__init__()\n        # Fixed typo: save_hyperparameters\n        self.save_hyperparameters(ignore=[\"net\"])\n        # Fixed: assigned 'net' instead of undefined 'model'\n        self.net_module = net\n        # Fixed: removed undefined 'config' object and used init args\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.out_channels = 2\n        self.spatial_dims  = 3\n        self.ignore_index_val = 2\n        \n        self.crossEntropy = nn.CrossEntropyLoss(\n            ignore_index = self.ignore_index_val\n        )\n        self.Tversky = TverskyLoss(\n            softmax = True,\n            to_onehot_y = False,\n            include_background = True,\n            alpha = 0.75,\n            beta = 0.25  \n        )\n\n    # Added missing forward method required for self(img)\n    def forward(self, x):\n        return self.net_module(x)\n\n    def _compute_loss(self, logits, targets):\n        # pytorch cross entropy can ignore classes inherintly\n        targets = targets.squeeze(1)\n        ce_loss = self.crossEntropy(logits, targets.long())\n        \n        # mask so tversky doesnt get confused by class 2 as it cant ignore it natively\n        # Fixed: Changed 'target' to 'targets'\n        mask = (targets != self.ignore_index_val)\n        \n        # setting index 2 to 0\n        # Fixed: Changed 'target' to 'targets'\n        target_clean = torch.where(mask, targets, torch.tensor(0, device=targets.device))\n        \n        targets_onehot = torch.nn.functional.one_hot(\n            target_clean.long(),\n            num_classes=2\n        ).float()\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        if self.spatial_dims == 3:\n            targets_onehot = targets_onehot.permute(0,4,1,2,3)\n        else:\n            targets_onehot = targets_onehot.permute(0,3,1,2) # Fixed permute for 2D\n        \n        # CHANGED: .half() -> .float() to prevent runtime errors on standard FP32 training\n        targets_masked_ohe = targets_onehot * mask.unsqueeze(1).float() \n\n        tversky_loss = self.Tversky(logits, targets_masked_ohe)\n\n        return 0.5*tversky_loss + 0.5*ce_loss\n\n    def _compute_metrics(self, preds_logits: torch.Tensor, targets_class_indices: torch.Tensor) -> dict:\n        \"\"\"\n        Computes Dice and IoU fully vectorized (No for-loops).\n        \"\"\"\n        num_classes = preds_logits.shape[1]\n        preds_hard = torch.argmax(preds_logits, dim=1, keepdim=True)\n        valid_mask = (targets_class_indices != self.ignore_index_val)\n        \n        # 3. One-Hot Encode Predictions\n        preds_ohe = torch.nn.functional.one_hot(\n            preds_hard.squeeze(1), \n            num_classes=num_classes\n        ).float()\n        \n        # 4. One-Hot Encode Targets\n        targets_clean = torch.where(valid_mask, targets_class_indices, torch.tensor(0, device=targets_class_indices.device))\n        \n        targets_ohe = torch.nn.functional.one_hot(\n            targets_clean.squeeze(1).long(), \n            num_classes=num_classes\n        ).float()\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        if self.spatial_dims == 3:\n            preds_ohe = preds_ohe.permute(0, 4, 1, 2, 3)\n            targets_ohe = targets_ohe.permute(0, 4, 1, 2, 3)\n        else:\n            preds_ohe = preds_ohe.permute(0, 3, 1, 2)\n            targets_ohe = targets_ohe.permute(0, 3, 1, 2)\n\n        valid_mask_float = valid_mask.float()\n        preds_ohe = preds_ohe * valid_mask_float\n        targets_ohe = targets_ohe * valid_mask_float\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        reduce_dims = (0, 2, 3, 4) if self.spatial_dims == 3 else (0, 2, 3)\n\n        intersection = (preds_ohe * targets_ohe).sum(dim=reduce_dims)\n        cardinality_pred = preds_ohe.sum(dim=reduce_dims)\n        cardinality_target = targets_ohe.sum(dim=reduce_dims)\n        \n        union_sum = cardinality_pred + cardinality_target\n\n        dice_scores = (2. * intersection + 1e-8) / (union_sum + 1e-8)\n        iou_scores = (intersection + 1e-8) / (union_sum - intersection + 1e-8)\n\n        return {\n            \"dice\": dice_scores.mean(),\n            \"iou\": iou_scores.mean()\n        }\n\n    # Added 'self' argument\n    def training_step(self, batch, batch_idx):\n        img, mask, _ = batch\n        logits = self(img)\n        loss = self._compute_loss(logits, mask)\n        # Fixed function name _compute_metric -> _compute_metrics\n        # Fixed variable name metric -> metrics (to match logging below)\n        metrics = self._compute_metrics(logits, mask)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_dice\", metrics[\"dice\"], on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_iou\", metrics[\"iou\"], on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n    # Added 'self' argument\n    def validation_step(self, batch, batch_idx):\n        img, mask, _ = batch\n        logits = self(img)\n        loss = self._compute_loss(logits, mask)\n        # Fixed function name _compute_metric -> _compute_metrics\n        # Fixed variable name metric -> metrics\n        metrics = self._compute_metrics(logits, mask)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_dice\", metrics[\"dice\"], on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_iou\", metrics[\"iou\"], on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    # Added 'self' argument and renamed to predict_step\n    def predict_step(self, batch, batch_idx):\n        img, _, frag_id = batch\n        logits = self(img)\n        prob = torch.softmax(logits, dim=1)\n        classes = torch.argmax(prob, dim=1)\n        return {\"prediction\": classes, \"frag_id\": frag_id}\n        \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            # Fixed MAX_EPOCHS to use self.trainer.max_epochs\n            T_max=self.trainer.max_epochs if self.trainer else 100,\n            eta_min=1e-6\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\"\n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.907315Z","iopub.status.idle":"2025-12-10T02:58:31.907703Z","shell.execute_reply.started":"2025-12-10T02:58:31.907511Z","shell.execute_reply":"2025-12-10T02:58:31.907529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.networks.nets import SegResNet, SwinUNETR\nnet = SwinUNETR(\n    in_channels=1,\n    out_channels=2,\n    feature_size=48,\n    use_v2=True,\n    drop_rate=0.2,\n    attn_drop_rate=0.2,\n    dropout_path_rate=0.2,\n    use_checkpoint=True\n)\n\nnet_name = net.__class__.__name__\nmodel = surfaceSegementation(net=net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.908787Z","iopub.status.idle":"2025-12-10T02:58:31.909136Z","shell.execute_reply.started":"2025-12-10T02:58:31.908956Z","shell.execute_reply":"2025-12-10T02:58:31.908972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpt_path = \"/kaggle/input/vesuvius-swinuneter-model1/checkpoints/SwinUNETR-epoch=29-val_dice=0.7091.ckpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.910876Z","iopub.status.idle":"2025-12-10T02:58:31.911742Z","shell.execute_reply.started":"2025-12-10T02:58:31.911578Z","shell.execute_reply":"2025-12-10T02:58:31.911606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pytorch_lightning as pl\n# from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n# from pytorch_lightning.loggers import CSVLogger\n# from pytorch_lightning.utilities.exceptions import MisconfigurationException\n\n# # Callbacks\n# checkpoint_callback = ModelCheckpoint(\n#     dirpath=OUTPUT_DIR,\n#     filename=net_name + \"-{epoch:02d}-{val_dice:.4f}\",\n#     monitor=\"val_dice\",\n#     mode=\"max\",\n#     save_top_k=3,\n#     verbose=True\n# )\n\n# early_stop_callback = EarlyStopping(\n#     monitor=\"val_dice\",\n#     patience=10,\n#     mode=\"max\",\n#     verbose=True\n# )\n\n# lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n# csv_logger = CSVLogger(save_dir=OUTPUT_DIR)\n\n# # Trainer\n# trainer = pl.Trainer(\n#     max_epochs=config.max_epochs,\n#     accelerator=\"auto\",\n#     devices=\"auto\",\n#     logger=csv_logger,\n#     callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n#     precision=\"16-mixed\",\n#     log_every_n_steps=20,\n#     enable_progress_bar=True,\n#     accumulate_grad_batches=18,\n#     gradient_clip_val=1.0, # Clips gradient norm to 1.0 to prevent exploding gradients\n# )\n\n# # Train\n# try:\n#     trainer.fit(model, datamodule=dataModule, ckpt_path=ckpt_path)\n# except MisconfigurationException as ex:\n#     print(ex)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.912433Z","iopub.status.idle":"2025-12-10T02:58:31.912937Z","shell.execute_reply.started":"2025-12-10T02:58:31.912759Z","shell.execute_reply":"2025-12-10T02:58:31.912776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nmetrics1 = pd.read_csv(\"/kaggle/input/vesuvius-swinuneter-model1/checkpoints/lightning_logs/version_0/metrics.csv\")\nmetrics1.head()\nmetrics1['epoch'].unique()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.914112Z","iopub.status.idle":"2025-12-10T02:58:31.914458Z","shell.execute_reply.started":"2025-12-10T02:58:31.914308Z","shell.execute_reply":"2025-12-10T02:58:31.914325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics2 = pd.read_csv(\"/kaggle/input/vesuvius-baseline-model/checkpoints/lightning_logs/version_0/metrics.csv\")\nmetrics2.head()\nmetrics2['epoch'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.915055Z","iopub.status.idle":"2025-12-10T02:58:31.915371Z","shell.execute_reply.started":"2025-12-10T02:58:31.915191Z","shell.execute_reply":"2025-12-10T02:58:31.915205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.916823Z","iopub.status.idle":"2025-12-10T02:58:31.917071Z","shell.execute_reply.started":"2025-12-10T02:58:31.916952Z","shell.execute_reply":"2025-12-10T02:58:31.916964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Load the two CSV files\n# Replace 'metrics_0_30.csv' and 'metrics_30_50.csv' with your actual file names\ndf1 = pd.read_csv('/kaggle/input/vesuvius-swinuneter-model1/checkpoints/lightning_logs/version_0/metrics.csv')\ndf2 = pd.read_csv('/kaggle/input/vesuvius-baseline-model/checkpoints/lightning_logs/version_0/metrics.csv')\n\n# 2. Concatenate them\n# ignore_index=True is important here: it resets the index so it runs continuously\n# from 0 to the end, instead of restarting at 0 for the second file.\nmerged_df = pd.concat([df1, df2], ignore_index=True)\n\n# 3. Check the result\nprint(f\"Shape of first file: {df1.shape}\")\nprint(f\"Shape of second file: {df2.shape}\")\nprint(f\"Shape of merged file: {merged_df.shape}\")\n\n# 4. Save the merged data to a new CSV\nmerged_df.to_csv('merged_metrics.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.917616Z","iopub.status.idle":"2025-12-10T02:58:31.917862Z","shell.execute_reply.started":"2025-12-10T02:58:31.917747Z","shell.execute_reply":"2025-12-10T02:58:31.917758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.919260Z","iopub.status.idle":"2025-12-10T02:58:31.919781Z","shell.execute_reply.started":"2025-12-10T02:58:31.919645Z","shell.execute_reply":"2025-12-10T02:58:31.919658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming your dataframe is named 'df'\n\n# 1. Clean the data by grouping by epoch\n# This merges rows so that for each epoch, you have the values for all metrics\n# .mean() works well here because the non-NaN values are usually unique per epoch\nepoch_df = merged_df.groupby('epoch').mean()\n\n# Reset index so 'epoch' becomes a column again for plotting\nepoch_df = epoch_df.reset_index()\n\n# 2. Setup the plots\nsns.set_theme(style=\"whitegrid\")\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\n# --- Plot 1: Dice Scores ---\n# using 'train_dice_epoch' vs 'val_dice'\naxes[0].plot(epoch_df['epoch'], epoch_df['train_dice_epoch'], label='Train Dice', marker='o')\naxes[0].plot(epoch_df['epoch'], epoch_df['val_dice'], label='Val Dice', marker='o')\naxes[0].set_title('Dice Score vs Epoch', fontsize=14)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Dice Score', fontsize=12)\naxes[0].legend()\n\n# --- Plot 2: Loss ---\n# using 'train_loss_epoch' vs 'val_loss'\naxes[1].plot(epoch_df['epoch'], epoch_df['train_loss_epoch'], label='Train Loss', marker='o')\naxes[1].plot(epoch_df['epoch'], epoch_df['val_loss'], label='Val Loss', marker='o')\naxes[1].set_title('Loss vs Epoch', fontsize=14)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Loss', fontsize=12)\naxes[1].legend()\n\n# --- Plot 3: IoU ---\n# using 'train_iou_epoch' vs 'val_iou'\naxes[2].plot(epoch_df['epoch'], epoch_df['train_iou_epoch'], label='Train IoU', marker='o')\naxes[2].plot(epoch_df['epoch'], epoch_df['val_iou'], label='Val IoU', marker='o')\naxes[2].set_title('IoU vs Epoch', fontsize=14)\naxes[2].set_xlabel('Epoch', fontsize=12)\naxes[2].set_ylabel('IoU', fontsize=12)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.921249Z","iopub.status.idle":"2025-12-10T02:58:31.921552Z","shell.execute_reply.started":"2025-12-10T02:58:31.921415Z","shell.execute_reply":"2025-12-10T02:58:31.921431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/input/vesuvius-baseline-model/checkpoints/SwinUNETR-epoch=47-val_dice=0.7371.ckpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.922917Z","iopub.status.idle":"2025-12-10T02:58:31.923353Z","shell.execute_reply.started":"2025-12-10T02:58:31.923122Z","shell.execute_reply":"2025-12-10T02:58:31.923140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel = surfaceSegementation.load_from_checkpoint(\n    checkpoint_path, \n    net=net\n)\n\n# 3. Prepare for Inference\nmodel.eval()        # Disables dropout/batchnorm updates\nmodel.freeze()      # Sets requires_grad=False to save memory\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T02:58:31.924848Z","iopub.status.idle":"2025-12-10T02:58:31.925346Z","shell.execute_reply.started":"2025-12-10T02:58:31.924993Z","shell.execute_reply":"2025-12-10T02:58:31.925008Z"}},"outputs":[],"execution_count":null}]}