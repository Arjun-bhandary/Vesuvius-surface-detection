{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":117682,"databundleVersionId":14443416},{"sourceType":"datasetVersion","sourceId":13802080,"datasetId":8787846,"databundleVersionId":14558224},{"sourceType":"datasetVersion","sourceId":14063070,"datasetId":8945494,"databundleVersionId":14843251},{"sourceType":"datasetVersion","sourceId":13818749,"datasetId":8800020,"databundleVersionId":14576516},{"sourceType":"kernelVersion","sourceId":284794863}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index --find-links=\"/kaggle/input/surface-package-scraper\" -q pytorch_lightning monai albumentations imagecodecs --no-deps # \"numpy==1.26.4\" \"scipy==1.15.3\"\n!pip uninstall -q -y tensorflow  # preventing AttributeError","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1 - 384\n\n\n40 - 256\n\n750 - 320","metadata":{"execution":{"iopub.status.busy":"2025-12-04T06:40:26.996809Z","iopub.execute_input":"2025-12-04T06:40:26.997143Z","iopub.status.idle":"2025-12-04T06:40:27.005267Z","shell.execute_reply.started":"2025-12-04T06:40:26.997122Z","shell.execute_reply":"2025-12-04T06:40:27.004282Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport gc\nimport warnings\nfrom typing import Tuple, Optional, Dict, List, Callable\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport os\nwarnings.filterwarnings(\"ignore\")\n\nimages_path = \"/kaggle/input/vesuvius-surface-npz/train_images\"\nmask_path = \"/kaggle/input/vesuvius-surface-npz/train_labels\"\nroot_dir = \"/kaggle/input/vesuvius-surface-npz\"\ntest_images_path = \"/kaggle/input/vesuvius-challenge-surface-detection/test_images\"\nOUTPUT_DIR = \"/kaggle/working/checkpoints\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.lr = 1e-4\n        self.num_workers = 2\n        self.batches = 1\n        self.val_split = 0.2\n        self.target_shape = (128,128,128)\n        self.weight_decay = 2e-4\n        self.max_epochs = 50\n        \n\nconfig = Config()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom pathlib import Path\nimport tifffile\nimport pytorch_lightning as pl\nimport random\nfrom monai import transforms as MT\n\n# --- 1. Modified Dataset ---\nclass VesuviusDataset(Dataset):\n    def __init__(self, \n                 images_id: list, \n                 mode: str = \"train\",\n                 transform=None): # Added transform argument\n        \n        self.images_id = images_id\n        self.mode = mode\n        self.images_dir = Path(images_path)\n        self.masks_dir = Path(masks_path) if masks_dir else None\n        self.test_dir = Path(test_images_path) if test_dir else None\n        self.transform = transform # Store the transform\n\n    def __len__(self):\n        return len(self.images_id)\n\n    def __getitem__(self, idx):\n        file_name = self.images_id[idx]\n        \n        # 1. Determine Source Directory\n        if self.mode == \"test\":\n            if self.test_dir is None:\n                raise ValueError(\"Mode is 'test' but no 'test_dir' was provided!\")\n            source_dir = self.test_images_path\n        else:\n            source_dir = self.test_images_path\n\n        # 2. Load Image (Returns Tensor)\n        img_path = source_dir / file_name\n        img = self.load_volume(img_path)\n\n        # 3. Load Mask\n        mask = None\n        if self.mode in [\"train\", \"val\"]:\n            if self.masks_dir is None:\n                raise ValueError(f\"Mode is '{self.mode}' but no 'masks_dir' was provided!\")\n            mask_path = self.masks_dir / file_name\n            mask = self.load_volume(mask_path, is_mask=True)\n\n        # --- 4. Apply Transforms (CPU Side) ---\n        # Wrap in dictionary for MONAI dictionary-based transforms\n        data = {\"image\": img}\n        if mask is not None:\n            data[\"label\"] = mask\n\n        if self.transform:\n            # Apply transform\n            data = self.transform(data)\n\n        # Unpack back to tuple\n        img_out = data[\"image\"]\n        mask_out = data[\"label\"] if \"label\" in data else None\n\n        return img_out, mask_out, file_name\n\n    def load_volume(self, file_path, is_mask=False):\n        path_obj = Path(file_path)\n        if not path_obj.exists():\n            raise FileNotFoundError(f\"Could not find file: {path_obj}\")\n        \n        if path_obj.suffix == \".npz\":\n            archive = np.load(str(path_obj))\n            data = archive[list(archive.files)[0]]\n        elif path_obj.suffix in [\".tif\", \".tiff\"]:\n            data = tifffile.imread(str(path_obj))\n        else:\n            data = np.load(str(path_obj))\n        \n        tensor = torch.from_numpy(data)\n\n        if not is_mask:\n            # Normalize and add channel dim\n            tensor = tensor.half().div_(255.0).unsqueeze(0)\n        else:\n            # Add channel dim\n            tensor = tensor.long().unsqueeze(0)\n\n        return tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimages_id = None\nfor _,_,c in os.walk(images_path):\n    images_id = c\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate(batch):\n    return batch\n#we want colation on happen on gpu as cpu cant do heavy interpolation\n#skipping the collation in dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DesuviusDataModule(pl.LightningDataModule):\n    def __init__(self,\n                 num_workers=4,\n                 batches=8,\n                 target_shape=(96, 96, 96), # Ensure this matches your VRAM limits\n                 val_split=0.2):\n        super().__init__()\n        self.train_img_dir = images_path\n        self.test_img_dir= test_images_path\n        self.mask_img_dir = mask_path\n        self.num_workers = num_workers\n        self.batches = batches\n        self.target_shape = config.target_shape\n        self.val_split = val_split\n\n        # Define Transforms HERE (CPU)\n        # Note: EnsureTyped ensures we output Tensors, track_meta=False saves memory\n        self.train_transforms = MT.Compose([\n            MT.Resized(keys=[\"image\", \"label\"], spatial_size=self.target_shape, mode=[\"trilinear\", \"nearest\"]),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n            MT.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n            MT.RandRotated(keys=[\"image\", \"label\"], range_x=0.1, range_y=0.1, range_z=0.1, prob=0.3, keep_size=True, mode=[\"bilinear\", \"nearest\"]),\n            MT.RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n            MT.RandGaussianNoised(keys=[\"image\"], prob=0.3, mean=0.0, std=0.01),\n            MT.EnsureTyped(keys=[\"image\", \"label\"], track_meta=False) \n        ])\n\n        self.val_transforms = MT.Compose([\n            MT.Resized(keys=[\"image\", \"label\"], spatial_size=self.target_shape, mode=[\"trilinear\", \"nearest\"]),\n            MT.EnsureTyped(keys=[\"image\", \"label\"], track_meta=False)\n        ])\n\n        self.test_transforms = MT.Compose([\n            MT.Resized(keys=[\"image\"], spatial_size=self.target_shape, mode=[\"trilinear\"]),\n            MT.EnsureTyped(keys=[\"image\"], track_meta=False)\n        ])\n\n    def setup(self, stage=None):\n        random.seed(32)\n        # Assuming 'images_id' is passed in or defined globally. \n        # Ideally, pass this into __init__\n        all_files = images_id \n        random.shuffle(all_files)\n        \n        split_idx = int(len(all_files) * (1 - self.val_split))\n        train_files = all_files[:split_idx]\n        val_files = all_files[split_idx:]\n        \n        print(f\"Total files: {len(all_files)}\")\n        print(f\"Train files: {len(train_files)}\")\n        print(f\"Val files: {len(val_files)}\")\n\n        # Initialize Datasets WITH transforms\n        self.train_dataset = VesuviusDataset(\n            images_id=train_files, \n            images_dir=self.train_img_dir,\n            masks_dir=self.mask_img_dir,\n            mode=\"train\", \n            transform=self.train_transforms\n        )\n        \n        self.val_dataset = VesuviusDataset(\n            images_id=val_files, \n            images_dir=self.train_img_dir,\n            masks_dir=self.mask_img_dir,\n            mode=\"val\", \n            transform=self.val_transforms\n        )\n        \n        # If you have a test set logic, initialize it here with self.test_transforms\n\n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batches,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True, # Critical for speed when using CPU transforms\n            persistent_workers=bool(self.num_workers > 0),\n            # collate_fn=custom_collate # Use if you have a specific need, otherwise default works\n        )\n\n    def val_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batches,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=bool(self.num_workers > 0),\n            # collate_fn=custom_collate \n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataModule = DesuviusDataModule()\ndataModule.setup()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataModule.train_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get a batch\ntrain_loader = dataModule.train_dataloader()\nbatch = next(iter(train_loader))\n\n# 2. Unpack the batch\n# With default collation, PyTorch stacks them into tensors: (Batch, Channel, D, H, W)\nimages_batch, masks_batch, frag_ids = batch\n\n# Select the first sample in the batch\n# Shape is (C, D, H, W), usually (1, 96, 96, 96)\nraw_img_tensor = images_batch[0] \nraw_mask_tensor = masks_batch[0]\nfrag_id = frag_ids[0]\n\nprint(f\"ID: {frag_id}\")\nprint(f\"Tensor Shape (with channel): {raw_img_tensor.shape}\")\n\n# 3. Prepare for Plotting\n# We don't need to apply transforms manually anymore; they are already applied!\n# Just extract Channel 0 and convert to Numpy.\nimg = raw_img_tensor[0].detach().cpu().numpy()  # (D, H, W)\nmsk = raw_mask_tensor[0].detach().cpu().numpy() # (D, H, W)\n\nprint(f\"Plotting Shape: {img.shape}\")\nprint(f\"Image Range: {img.min():.2f} - {img.max():.2f}\")\n\n# 4. Visualization Logic\n# Calculate middle indices\nd_mid = img.shape[0] // 2\nh_mid = img.shape[1] // 2\nw_mid = img.shape[2] // 2\n\n# Setup plot: 3 Rows (Axes), 2 Cols (Image, Mask)\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\n# Row 1: Z-axis (Depth/Axial)\naxes[0, 0].imshow(img[d_mid, :, :], cmap='gray')\naxes[0, 0].set_title(f'Axial (Depth={d_mid}) - Image')\naxes[0, 1].imshow(msk[d_mid, :, :], cmap='gray')\naxes[0, 1].set_title(f'Axial (Depth={d_mid}) - Mask')\n\n# Row 2: Y-axis (Height/Coronal)\naxes[1, 0].imshow(img[:, h_mid, :], cmap='gray')\naxes[1, 0].set_title(f'Coronal (Height={h_mid}) - Image')\naxes[1, 1].imshow(msk[:, h_mid, :], cmap='gray')\naxes[1, 1].set_title(f'Coronal (Height={h_mid}) - Mask')\n\n# Row 3: X-axis (Width/Sagittal)\naxes[2, 0].imshow(img[:, :, w_mid], cmap='gray')\naxes[2, 0].set_title(f'Sagittal (Width={w_mid}) - Image')\naxes[2, 1].imshow(msk[:, :, w_mid], cmap='gray')\naxes[2, 1].set_title(f'Sagittal (Width={w_mid}) - Mask')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pytorch_lightning as pl\nfrom monai.losses import TverskyLoss\n\nclass surfaceSegementation(pl.LightningModule):\n    def __init__(self, net, learning_rate=1e-3, weight_decay=1e-4):\n        super().__init__()\n        # Fixed typo: save_hyperparameters\n        self.save_hyperparameters(ignore=[\"net\"])\n        # Fixed: assigned 'net' instead of undefined 'model'\n        self.net_module = net\n        # Fixed: removed undefined 'config' object and used init args\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.out_channels = 2\n        self.spatial_dims  = 3\n        self.ignore_index_val = 2\n        \n        self.crossEntropy = nn.CrossEntropyLoss(\n            ignore_index = self.ignore_index_val\n        )\n        self.Tversky = TverskyLoss(\n            softmax = True,\n            to_onehot_y = False,\n            include_background = True,\n            alpha = 0.75,\n            beta = 0.25  \n        )\n\n    # Added missing forward method required for self(img)\n    def forward(self, x):\n        return self.net_module(x)\n\n    def _compute_loss(self, logits, targets):\n        # pytorch cross entropy can ignore classes inherintly\n        targets = targets.squeeze(1)\n        ce_loss = self.crossEntropy(logits, targets.long())\n        \n        # mask so tversky doesnt get confused by class 2 as it cant ignore it natively\n        # Fixed: Changed 'target' to 'targets'\n        mask = (targets != self.ignore_index_val)\n        \n        # setting index 2 to 0\n        # Fixed: Changed 'target' to 'targets'\n        target_clean = torch.where(mask, targets, torch.tensor(0, device=targets.device))\n        \n        targets_onehot = torch.nn.functional.one_hot(\n            target_clean.long(),\n            num_classes=2\n        ).float()\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        if self.spatial_dims == 3:\n            targets_onehot = targets_onehot.permute(0,4,1,2,3)\n        else:\n            targets_onehot = targets_onehot.permute(0,3,1,2) # Fixed permute for 2D\n        \n        # CHANGED: .half() -> .float() to prevent runtime errors on standard FP32 training\n        targets_masked_ohe = targets_onehot * mask.unsqueeze(1).float() \n\n        tversky_loss = self.Tversky(logits, targets_masked_ohe)\n\n        return 0.5*tversky_loss + 0.5*ce_loss\n\n    def _compute_metrics(self, preds_logits: torch.Tensor, targets_class_indices: torch.Tensor) -> dict:\n        \"\"\"\n        Computes Dice and IoU fully vectorized (No for-loops).\n        \"\"\"\n        num_classes = preds_logits.shape[1]\n        preds_hard = torch.argmax(preds_logits, dim=1, keepdim=True)\n        valid_mask = (targets_class_indices != self.ignore_index_val)\n        \n        # 3. One-Hot Encode Predictions\n        preds_ohe = torch.nn.functional.one_hot(\n            preds_hard.squeeze(1), \n            num_classes=num_classes\n        ).float()\n        \n        # 4. One-Hot Encode Targets\n        targets_clean = torch.where(valid_mask, targets_class_indices, torch.tensor(0, device=targets_class_indices.device))\n        \n        targets_ohe = torch.nn.functional.one_hot(\n            targets_clean.squeeze(1).long(), \n            num_classes=num_classes\n        ).float()\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        if self.spatial_dims == 3:\n            preds_ohe = preds_ohe.permute(0, 4, 1, 2, 3)\n            targets_ohe = targets_ohe.permute(0, 4, 1, 2, 3)\n        else:\n            preds_ohe = preds_ohe.permute(0, 3, 1, 2)\n            targets_ohe = targets_ohe.permute(0, 3, 1, 2)\n\n        valid_mask_float = valid_mask.float()\n        preds_ohe = preds_ohe * valid_mask_float\n        targets_ohe = targets_ohe * valid_mask_float\n\n        # Fixed: Changed self.hparams.spatial_dims to self.spatial_dims\n        reduce_dims = (0, 2, 3, 4) if self.spatial_dims == 3 else (0, 2, 3)\n\n        intersection = (preds_ohe * targets_ohe).sum(dim=reduce_dims)\n        cardinality_pred = preds_ohe.sum(dim=reduce_dims)\n        cardinality_target = targets_ohe.sum(dim=reduce_dims)\n        \n        union_sum = cardinality_pred + cardinality_target\n\n        dice_scores = (2. * intersection + 1e-8) / (union_sum + 1e-8)\n        iou_scores = (intersection + 1e-8) / (union_sum - intersection + 1e-8)\n\n        return {\n            \"dice\": dice_scores.mean(),\n            \"iou\": iou_scores.mean()\n        }\n\n    # Added 'self' argument\n    def training_step(self, batch, batch_idx):\n        img, mask, _ = batch\n        logits = self(img)\n        loss = self._compute_loss(logits, mask)\n        # Fixed function name _compute_metric -> _compute_metrics\n        # Fixed variable name metric -> metrics (to match logging below)\n        metrics = self._compute_metrics(logits, mask)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_dice\", metrics[\"dice\"], on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_iou\", metrics[\"iou\"], on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n    # Added 'self' argument\n    def validation_step(self, batch, batch_idx):\n        img, mask, _ = batch\n        logits = self(img)\n        loss = self._compute_loss(logits, mask)\n        # Fixed function name _compute_metric -> _compute_metrics\n        # Fixed variable name metric -> metrics\n        metrics = self._compute_metrics(logits, mask)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_dice\", metrics[\"dice\"], on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_iou\", metrics[\"iou\"], on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    # Added 'self' argument and renamed to predict_step\n    def predict_step(self, batch, batch_idx):\n        img, _, frag_id = batch\n        logits = self(img)\n        prob = torch.softmax(logits, dim=1)\n        classes = torch.argmax(prob, dim=1)\n        return {\"prediction\": classes, \"frag_id\": frag_id}\n        \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            # Fixed MAX_EPOCHS to use self.trainer.max_epochs\n            T_max=self.trainer.max_epochs if self.trainer else 100,\n            eta_min=1e-6\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\"\n            }\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.networks.nets import SegResNet, SwinUNETR\nnet = SwinUNETR(\n    in_channels=1,\n    out_channels=2,\n    feature_size=48,\n    use_v2=True,\n    drop_rate=0.2,\n    attn_drop_rate=0.2,\n    dropout_path_rate=0.2,\n    use_checkpoint=True\n)\n\nnet_name = net.__class__.__name__\nmodel = surfaceSegementation(net=net)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpt_path = \"/kaggle/input/vesuvius-swinuneter-model1/checkpoints/SwinUNETR-epoch=01-val_dice=0.6019.ckpt\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\n\n# Callbacks\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=OUTPUT_DIR,\n    filename=net_name + \"-{epoch:02d}-{val_dice:.4f}\",\n    monitor=\"val_dice\",\n    mode=\"max\",\n    save_top_k=3,\n    verbose=True\n)\n\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_dice\",\n    patience=10,\n    mode=\"max\",\n    verbose=True\n)\n\nlr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\ncsv_logger = CSVLogger(save_dir=OUTPUT_DIR)\n\n# Trainer\ntrainer = pl.Trainer(\n    max_epochs=config.max_epochs,\n    accelerator=\"auto\",\n    devices=\"auto\",\n    logger=csv_logger,\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n    precision=\"16-mixed\",\n    log_every_n_steps=20,\n    enable_progress_bar=True,\n    accumulate_grad_batches=18,\n    gradient_clip_val=1.0, # Clips gradient norm to 1.0 to prevent exploding gradients\n)\n\n# Train\ntry:\n    trainer.fit(model, datamodule=dataModule, ckpt_path=ckpt_path)\nexcept MisconfigurationException as ex:\n    print(ex)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}